### 链表的由来

我们知道，内存大小是有限的，数组对内存的要求又是连续的。假设我们存储一个比较大的数据（比如1.5G），而此时申请的内存中没有这么大的连续内存，我们该怎么办呢，一个办法是扩容，但这涉及到数据搬移，非常耗性能，而且可能会造成数据丢失。链表则很好的解决了这个问题，链表不要求连续内存，因此可以充分利用零散内存，当然这也导致了链表不利于缓存，查询效率低。



### 链表的特点

链表和数组作为最基本的存储结构，介绍链表的特点需要对比数组，如下：

+ 存储上：连续与非连续。数组需要申请一段连续的内存；链表不必是连续的空间，内存利用率高。
+ 操作上：插入、删除、查询的时间复杂度。数组利用下标查询的时间复杂度为O(1)，但插入和删除的平均时间复杂度为O(n)；链表的插入和删除的时间复杂度为O(1)，查询的时间复杂度为O(n)。
+ 缓存上：链表不太适合缓存。



关于链表的分类，这里分为下面四种，其实第四种可以归并到前面几种中，这里单独抽离出来介绍其用法：

+ 单向链表
+ 双向链表
+ 循环链表（约瑟夫问题）
+ 有序链表



### 链表适合解决什么问题

上面介绍了链表分类，对比单向链表，单向链表的插入、删除操作的时间复杂度已经是 O(1) 了，那么双向链表的优势在哪？其实这种说法不是很准确。

以**删除**操作为例，从链表中删除一个数据无外乎这两种情况：

+ 删除结点中“值等于某个给定值”的结点；
+ 删除给定指针指向的结点。

对于第一种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。

尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。



对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p->next=q，说明 p 是 q 的前驱结点。

但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要 O(n) 的时间复杂度，而双向链表只需要在 O(1) 的时间复杂度内就搞定了！



同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在 O(1) 时间复杂度搞定，而单向链表需要 O(n) 的时间复杂度。



这其中涉及了空间换时间和时间换空间思想。对于执行较慢的程序，可以通过缓存（空间换时间）进行优化；对于内存消耗过快的程序，可以通过消耗更多时间（时间换空间）进行优化；



### 实际的应用场景（结合代码实现）

关于链表，最经典的一个应用场景是LRU 缓存淘汰算法，有两种实现思路：
+ 基于链表的基本实现O(n)
+ 散列表+双向链表优化O(1)



**基于链表的基本实现**

我们维护一个有序链表，越靠近链表尾部的是越早被访问的，当有一个新数据被访问时，需要从链表头部开始遍历。

1. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，我们需要将其从原来的位置删除，然后插入到链表的开头
2. 如果此数据没有在缓存链表中，又分为两种情况：
   + 如果此时缓存未满，则将此结点直接插入到链表的头部；
   + 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。



**散列表+双向链表优化**

在上面**基于链表的基本实现**中，已经实现了基本的LRU缓存，但是时间复杂度为O(n)，那如何继续进行优化呢？我们知道通过**散列表**

访问数据的时间复杂度是O(1)，因此可以结合散列表和双向链表。

<img src="../imgs/基于散列表实现LRU缓存.jpg" style="zoom:50%;" />



### 思考

+ 在介绍链表特点的时候，为什么说数组使用连续内存存储，更利于CPU的缓存，而链表不需要连续内存存储，则不利于CPU的缓存呢？
> CPU在从内存读取数据的时候，会先把读取到的数据加载到CPU的缓存中，然后再读到寄存器中。而CPU每次从内存读取数据并不是只读取那个特定要访问的地址，而是读取一个数据块，并保存到CPU缓存中，然后下次访问内存数据的时候就会先从CPU缓存开始查找，如果找到就不需要再从内存中取。这样就实现了比内存访问速度更快的机制，也就是CPU缓存存在的意义:为了弥补内存访问速度过慢与CPU执行速度快之间的差异而引入。

对于数组来说，存储空间是连续的，所以在加载某个下标的时候可以把以后的几个下标元素也加载到CPU缓存这样执行速度会快于存储空间不连续的链表存储。